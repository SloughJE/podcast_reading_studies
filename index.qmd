---
title: "Understanding Health & Nutrition Studies"
subtitle: "A Practical Framework for Understanding Scientific Studies"
author: "John Slough"
format:
  revealjs:
    self-contained: true
    theme: [night]
    slide-number: true
    css: styles.css
---


## Health and nutrition research is messy.

::: {.fragment}

* Even the best studies have limits  

* Headlines and influencers amplify confusion  

* Understanding study design, basic stats, and common traps cuts through the noise

:::

::: {.notes}

• “No study is perfect: sample size, confounders, and chance all leave room for error.”
- some studies are just bad.
• “Influencers and headlines or TikTok, don't leave room for nuance and often do not report what the study actually says.”  
• “we’ll learn more about scientific studies and stats basics so you can spot the signal behind the clickbait.”  

:::

---

## Overview 


1. **The Research Question**  
2. **Study Design**  
3. **Study Population**  
4. **Paper Structure**  
5. **Reporting Metrics**
6. **Interpreting Results**  
7. **Controlling Confounders**  
8. **Misleading Studies & Headlines**

::: {.notes}
* **1 The Research Question** – What is the study trying to answer?  
* **2 Study Design** – How was the evidence gathered and analysed?  
* **3 Study Population** – Who or what was studied?  
* **4 Paper Structure** – Where to find key information.  
* **5 Reporting Metrics** – Common measures in studies.  
* **6 Interpreting Results** – P-values, confidence intervals, statistical significance.  
* **7 Controlling Confounders** – Adjustment, confounding and a little math.  
* **8 Misleading Studies & Headlines** – Typical pitfalls you’ll spot in the wild.
:::

---

## The Research Question

::: {.smaller2}

**Main goal of the study?**

- Drives design, statistics, sample size  
- Clarify *primary* vs. *secondary* outcomes

<br>

**Example (published RCT)**  
"We tested the hypothesis that long-term supplementation with n-3 fatty acids would reduce cardiovascular events in this population."  
— [NEJM, 2012](https://www.nejm.org/doi/10.1056/NEJMoa1203859)

:::

::: {.notes}
**What is the study trying to answer?**  
Every paper begins with a research question or hypothesis.  
That single goal shapes the design, determines which statistics are run, and dictates how many participants are needed.


**Primary vs. secondary outcomes**  
*Primary* = the outcome the trial is powered to detect.  
*Secondary* = extra measurements, often exploratory.  
Important: An exploratory study can be valuable, but don’t confuse it with a trial designed to prove causality.

:::

---

## The Research Question

::: {.smaller2}

### Questions to Ask

- What specific question is the study trying to answer?

- Is the primary outcome clearly stated and appropriate?

- Are the secondary outcomes treated as exploratory, or being oversold?

- Does the goal match what you care about?

:::

---

## Study Design

::: {.smaller2}

What a study can claim depends on its design.

<br>

:::: {.columns}

::: {.column .fragment
      style="width:33%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#6bc7ff;">Observational</span>  
• Cross-sectional  
• Prospective cohort  
• Case-control
:::

::: {.column .fragment
      style="width:30%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#fdd663;">Interventional</span>  
• RCT   
• Pre–post (single-arm)



:::

::: {.column .fragment
      style="width:28%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             line-height:1.2;"}

### <span style="color:#a1e56e;">Evidence Synthesis</span>  
• Systematic review    
• Meta-analysis   



:::
::::

:::

::: {.notes}
**Key message**  
- *Observational* → explores associations.  
- *Interventional* → can test causality.  
- *Synthesis* → pools or summarises evidence for stronger estimates.

**Observational**  
- **Cross-sectional** – snapshot at one point in time.  
- **Prospective cohort** – follow exposure over time to see who develops the outcome.  
- **Case-control** – compare people with vs. without the outcome, look backward for exposures.  

→ Good for prevalence, trends, hypothesis generation; cannot by themselves prove cause.

**Interventional**  
- **Pre–post / single-arm** – measure before vs. after in the same group.  
- **Randomized Controlled Trial** – participants randomly assigned; balances confounders.  

→ Designed to answer causal questions and estimate treatment effects.

**Evidence Synthesis**  
- **Systematic review** – structured search + critical appraisal of all relevant studies.  
- **Meta-analysis** – statistical pooling step inside a systematic review (if data allow).  

→ Top of the evidence hierarchy when built on well-done RCTs; still limited by the quality and consistency of those underlying studies.
:::

---

## Other Study Designs (reference) {.smaller2}

<br>

:::: {.columns}
::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Observational**

• Case-control – with vs without outcome  
• Case report / series – detailed look at few patients  
• Ecological – group-level data only  
• Retrospective cohort – past records follow exposure → outcome  

<br>

**Mixed / Natural**

• Longitudinal – same subjects over time  
• Natural experiment – exposure assigned by external factors  
:::

::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Interventional**

• N-of-1 trial – single participant, alternating treatments  
• Cross-over trial – each subject receives all treatments  
• Before–after study – compare pre- vs post-intervention  

<br>

**Synthesis**

• Systematic review – structured summary, no pooling  
• Umbrella review – review of systematic reviews  
:::
::::

::: {.notes}
This is a quick reference list.  
I’ll mention that these designs exist but won’t unpack them here; listeners can pause the replay or check the show notes if they’d like more detail.
:::

## Study Design 

::: {.smaller2}

### Questions to Ask

- What type of study is this — observational, interventional, or a synthesis?

- Can this design prove causality, or only suggest associations?

- Is the comparison group (if any) appropriate and meaningful?

- For meta-analyses: Were the included studies similar enough to combine?

:::

---

## Who or What Is Being Studied?

**Population matters for relevance**  
_Not all studies are done on humans, and not all humans are like you._


- **Cells (in vitro)**
- **Animals (in vivo)**
- **Humans (in vivo)**


::: {.notes}
**Key idea**  

**The subjects of the study affect how applicable the results are.**  

The subjects tell you how far the findings can travel—cells → animals → humans is a “mechanistic → translational → applied” ladder.

**Cells (in vitro)**  
• Isolated cells in dishes or test tubes.  
• Great for uncovering biological mechanisms, drug targets, or toxicity pathways.  
• Cannot predict whole-body effects on their own.

**Animals (in vivo)**  
• Usually mice or rats; sometimes larger mammals.  
• Provide whole-organism context and dosing clues.  
• Differences in metabolism and lifespan still limit direct translation to people.

**Humans (in vivo)**  
• Ultimately most relevant, but generalisability varies by:  
  – Age, sex, health status  
  – Baseline diet, lifestyle, genetics  
• Always ask: “How close is this cohort to me or my patients?”

**Illustrative mismatch**  
A high-protein diet trial on *elderly Japanese women with osteoporosis undergoing survival training* probably says little about a 35-year-old office worker in Madrid.

**Take-home line**  
Before judging a result, check *who* or *what* was studied and whether that population resembles the one you care about.
:::

---

## Who or What Is Being Studied? 

### Questions to Ask

- What type of subjects were studied?  

- Why were they chosen?

- How relevant are they to you?

---

## Structure of a Scientific Paper 


1. **Abstract**  
2. **Introduction**  
3. **Methods**  
4. **Results**  
5. **Discussion**  
6. **Conclusion**  
7. **Supplementary**

::: {.notes}
**Abstract** – One-paragraph snapshot: question, design, key findings, and take-home.  
**Introduction** – Background, gap in knowledge, explicit research question, and why it matters.  
**Methods** – Who/what was studied, how data were gathered, study design, statistics used; lets you judge rigour and reproducibility.  
**Results** – Objective numbers: tables, figures, raw effect sizes. No interpretation here.  
**Discussion** – Authors’ interpretation, context with past work, strengths, limitations, and speculation.  
**Conclusion** – Concise answer to the research question; sometimes folded into Discussion.  
**Supplementary** – Extra data, detailed protocols, code, or extended figures that wouldn’t fit in the main text.

*Talking cue:* Emphasise that Methods and Results are where the “meat” lives; if those sections are thin, proceed with caution.
:::


---

## Structure of a Scientific Paper 

### Questions to Ask

- Does the Abstract summarize what is actually in the study?

- Does the Methods section clearly explain how the study was done?

- Is the data and statistics in the Results presented clearly?

- Does the Discussion align with the actual findings?


---

## How Results Are Reported

#### Population Metrics

::: {.fragment}
- **Counts** – raw tally
:::
::: {.fragment}
- **Percent / Proportion** – share of the population
:::
::: {.fragment}
- **Rates** – cases per person-time
:::
::: {.fragment}
- **Standardised rates** – adjusted for age / other factors
:::
::: {.fragment}
- **Incidence vs prevalence** – new vs total cases
:::


::: {.notes}
**Counts** – e.g. 500 heart-disease cases.  
**Percent / Proportion** – share of the population, e.g. 5 % prevalence.  
**Rates** – events per person-time, e.g. 10 per 1 000 person-years.  
**Standardised rates** – adjusted for age or other demographics to compare populations fairly.  
**Incidence vs. prevalence** – new cases in a period vs. total existing cases.
:::

---

## How Results Are Reported 

**Group difference**

- Percent change  
- Mean difference  

::: {.fragment}

**Risk comparison**

- Absolute risk (AR)  
- Relative risk (RR)  
- Odds ratio (OR)  
- Number Needed to Treat (NNT)  

:::
::: {.notes}
*Percent change* – relative shift from baseline (10% drop).  
*Mean difference* – raw unit gap (−5 mm Hg).  

*Absolute risk* – direct percentage-point change (2% → 1%).  
*Relative risk* – ratio of probabilities (RR 1.5 = 50% higher).  
*Odds ratio* – ratio of odds; close to RR when outcome is rare.  
*NNT* – number of people who must receive the treatment for one to benefit (100 means large trials needed).
:::

---

## How Results Are Reported 

### Time-to-Event Metrics

- **Incidence rate**  
- **Hazard ratio (HR)**  
- **Survival curves** 

::: {.fragment}


### Metrics in Meta-analyses

- Forest plot
- Standardized Mean Difference


:::

::: {.notes}
*Incidence rate* – new cases per person-time (5 per 1 000 py).  
*Hazard ratio* – relative event rate over time (HR 0.75 = 25 % lower).  
*Survival curves* – Kaplan–Meier plot of event-free probability.  

*Meta-analysis* – combines studies; forest plot shows each effect and pooled “diamond.”  
*SMD* – standardised mean difference; useful when studies use different units.
:::

---

## How Results Are Reported 

::: {.smaller2}

### Questions to Ask

- What type of metric is reported (count, effect difference, risk, rate, etc.)?

- Is that the right choice for this type of data, or could it be misleading because of population growth, time, or other factors?

- Does this metric directly answer the study’s main question?

- Could the way the result is presented exaggerate or downplay the real impact?

- What does the chosen metric reveal, and what might it obscure?

:::

---

## P-values & Confidence Intervals

::: {.smaller2}

<br>

**Is this effect real? How exact is it?**

<br>

| Concept                                  | What it tells us                  | Quick example                                   |
|------------------------------------------|-----------------------------------|-------------------------------------------------|
| **Statistical significance** *(p-value)* | Chance vs. real effect?           | Supplement ↓ BP **5 mm Hg**, *p = 0.03*          |
| **Precision** *(95 % CI)*                | How exact is the estimate?        | 5 mm Hg (**CI −8 to −2**)                        |
:::

::: {.notes}
### Why we need these measures  

Studies usually measure a sample, not the entire population.
Random variation in the sample can produce differences, even if there’s no real effect.
If we had data from the full population, we wouldn’t need p-values or confidence intervals.

*P-values* test whether those differences are likely due to chance; *confidence intervals* show how precisely we’ve pinned down the effect size.

### Key definitions
*P-value* – probability of obtaining data this extreme **assuming** the null-effect model is true.  
*95 % CI* – range that would capture the true effect in ~95 out of 100 identical studies.

> **ASA informal definition**  
> “A p-value is the probability, under a specified statistical model, that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” — American Statistical Association, 2016

**Notes to hit verbally:**  
* Threshold test: conventionally *p < 0.05* marks ‘statistically significant,’ but the cutoff is arbitrary.  
* p-value ≠ probability the effect is real.  
* Narrow CI ⇒ precise estimate; wide CI ⇒ more uncertainty.  
* A huge sample can make trivial effects “significant”; always couple p with effect size & CI.  
* Once p is well below the cutoff (e.g., 0.0003), extra decimal places add no meaning.
:::

---

## Interpreting the p-value 

- **Threshold test** → typically **p < 0.05**  
- Smaller p → stronger evidence **against** chance  

::: {.fragment}

- A *p*-value means little unless you also know the **effect size**

:::
::: {.fragment}

- **p ≠ probability the result is true**

:::
::: {.fragment}

> “5 mm Hg decrease, p = 0.03" → significant

:::

::: {.notes}
• Threshold is conventional, not magical; 0.01 or 0.10 have been used historically.  
• Highly significant (p < 0.001) values are often rounded because extra zeros don’t change interpretation.  
• Origin of 0.05 and common misuses go here.  
• Mention that p alone can’t judge importance—need effect size & CI.
:::

___

## Statistical vs. Practical Significance

::: {.smaller2}

<br>

| Finding (effect)             | p-value | Interpretation                                |
|------------------------------|:---------:|-----------------------------------------------|
| −0.15 kg (12 wk)             | 0.001  | **Statistically significant**; clinically trivial |
| −4.5 mm Hg systolic BP       | 0.09   | **Not statistically significant**; could matter if real |

<br>

:::

::: {.notes}
* A small, precise change can be meaningless in practice (−0.15 kg won’t alter anyone’s health plan).  
* A larger, borderline-non-significant effect may still matter—study could be under-powered, so look at CI width.  
* Always pair the p-value with effect size **and** CI before making clinical or lifestyle recommendations.  
* *“Would I change a guideline or my own behaviour over this number?”*
:::


---

## How to Interpret Results 

::: {.smaller2}

### Questions to Ask

- Is the result statistically significant, and how close is the p-value to the 0.05 threshold?

- What does the confidence interval tell me about the size and precision of the effect?

- If the result is statistically significant, is the result practically important?

- If the result is close to significant, could it be due to chance, small sample size, selective reporting, or fishing through lots of outcomes?

:::

---

## "Controlling for"? 

**Goal:** isolate the effect of one variable while accounting for others.

> Coffee & Heart Disease  
> Control for **smoking** so coffee isn’t blamed for smokers’ risk.

How?  

**Include the other influences in the statistical model.**

::: {.notes}
“When you read ‘we controlled for education, BMI, and smoking,’ it means those factors **were included as variables in the model** so we can ask:  
*If two people were identical on those factors, does coffee still matter?*”
:::

---

## "Controlling for"?

::: {.smaller2}

#### Example Cohort Study

**Question:** Does coffee raise heart-disease risk?

We record coffee cups/day, smoking, age, and heart-disease outcome.

<br> 

**Model with controls**  
`Heart disease = Coffee + Smoking + Age + error`  
→ Estimates the coffee effect *independent of* smoking & age

&nbsp;

**Model without smoking**  
`Heart disease = Coffee + Age + error`  
→ Smoking still influences both coffee & disease → **confounding**

:::

::: {.notes}
Leaving a major confounder out biases the coffee coefficient toward the smoking effect.
:::

---

## "Controlling for"?

::: {.smaller2}

### Under the Hood 

<br>

$$
Y = \alpha \;+\; \beta_1 X_{coffee} \;+\; \beta_2 X_{smoke} \;+\; \beta_3 X_{age} \;+\; \varepsilon
$$

| Symbol            | Plain English                                           |
|-------------------|---------------------------------------------------------|
| $Y$               | Outcome (heart disease)                                 |
| $\alpha$          | Baseline when all X = 0                                 |
| $\beta_{1,2,3}$   | Effect of each variable **holding the others constant** |
| $\varepsilon$     | Random noise / unexplained variation                    |

::: {.notes}
“The Greek letters just stand in for ‘effect of each predictor.’”
:::

:::

---

## “Controlling for”?


* Works **only** for variables we measured and included.  
* Unmeasured confounding can still bias results.  
* **Randomized trials** help by balancing both known and unknown confounders.

<small>More math? See the linked [PSU resource](https://online.stat.psu.edu/stat462/node/131/)</small>

::: {.notes}
“Adjustment is powerful but never perfect. Always ask what might be missing.”
:::


---

## "Controlling for"?

::: {.smaller2}

### Questions to Ask

- Did the study adjust for major known confounders (like smoking, age, BMI)?

- Are there important factors they might have missed or not measured?

- Is the adjustment described clearly, or does it seem vague or incomplete?

- Was the study randomized? (If yes, that helps handle unknown confounders too.)

:::

---

## Studies Can Mislead


- **Small samples inflate effects**  
- **Placebo isn’t inert**  
- **Complex Models & Subgroups Analyses**  
- **Funding or pet theories**
- **Multiple comparisons (p-hacking)**  
- **Self-reported or recall data**

::: {.notes}
**Small samples inflate effects** – Big swings occur by chance; replication shrinks them.  
**Placebo isn’t inert** – “Control” oils, shakes, or pills often have biological activity.  
**Over-fitted/fragmented models** – Too many variables or subgroup dives can turn noise into a ‘finding.’  
**Funding / pet theories** – Sponsor or author bias can frame questions or spin conclusions; doesn’t invalidate, but requires scrutiny.
**Multiple comparisons / p-hacking** – Testing dozens of outcomes raises the odds of a “significant” result by luck; preregistration helps curb this.  
**Self-reported or recall data** – Diet, exercise, or symptoms often logged from memory; measurement error can blur real associations.

:::

---

## Influencers Do Mislead 


- **One study ≠ truth**  
- **Click-bait abstracts**  
- **Publication bias**  
- **Petri-dish hype**  
- **Diet ≠ morality**  
- **Relative risk without absolute numbers**  
- **Association ≠ causation**

::: {.notes}
**One study ≠ truth** – Single papers are rarely definitive; replication matters.  
**Click-bait abstracts** – Summaries often overstate findings versus the body text.  
**Publication bias** – Null results stay in file drawers; positive ones get published. 
Perhaps you'd take fewer supplements if all the no-effect studies were published.
**Petri-dish hype** – Cell/animal results touted as human breakthroughs.  
**Diet ≠ morality** – Health claims sometimes smuggle in ideology or identity.  
**Relative risk without absolutes** – “50 % higher risk!” might be 2 cases → 3 cases per 10 000; influencers omit the baseline.
**Association ≠ causation** – Correlation alone can prompt fear or fads without causal proof. ie associations aren't the same level of evidence as causations. 
:::

---

## Core Questions to Ask 

::: {.smaller2}

<div style="font-size: 125%; line-height: 1.4;">

1. **Is the main question clear—and is it the one you care about?**  
2. **Study type:** can it speak to cause, or only association?  
3. **Who / what was studied—and how close is that to you?**  
4. **Metric chosen:** what does it reveal and what does it hide?  
5. **Result strength:** big, precise, and practically useful?  
6. **Red flags:** stats tricks, sensational claims, or conflicts?  

</div>

:::

::: {.notes}
1. Question clarity drives everything; if the paper drifts, stop.  
2. RCT or Mendelian randomization can test cause; cohort mainly spots links.  
3. Species, age, sex, health status—don’t apply mouse data to humans or elite-athlete data to office workers.  
4. Relative vs. absolute risks, mean vs. median—each tells a different story.  
5. Look at effect size *and* CI; a “significant” 0.15 kg loss isn’t life-changing.  
6. Tiny n, p-hacking, funding bias, or glitzy headlines demand extra caution.
:::

---

## Three Things to Remember {.smaller}

<div style="font-size: 127%; line-height: 1;">

1. **RCTs are the gold-standard for causality**  
   _Meta-analyses of well-run RCTs are even stronger._

<br>

2. **Cell & animal work is a first step, not a human verdict**
   _Petri-dish breakthroughs rarely translate 1:1 to people._
   
<br>

3. **Peek under the statistical hood**  
   _A result is only as strong as the methods and statistics behind it._

</div>

<div style="font-size:85%; color:#aaa; margin-top:0.8em;">
Good science takes longer than a 30-second TikTok.
</div>

::: {.notes}
Point 1 – “Gold-standard” avoids the absolute “only”; causal hints can come from natural experiments but need stronger assumptions.  
Point 2 – Petri-dish breakthroughs rarely translate 1:1 to people.  
Point 3 - headlines rarely mention the model, adjustments, or error checks, yet those determine whether a finding will hold up.

:::

