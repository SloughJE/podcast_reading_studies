---
title: "Decoding the Hype"
subtitle: "How to spot real evidence in health & nutrition studies"
author: "John for The John & Calvin Podcast"
format:
  revealjs:
    self-contained: true
    theme: [night]
    slide-number: true
    css: styles.css
---


## Health and nutrition research is messy.

::: {.fragment}

* No study is flawless. Even the best have limits.

* Headlines & influencers turn nuance into **noise and hot-takes**.  

:::
::: {.fragment}

* **The toolkit:** study design • stats basics • common pitfalls  


:::

::: {.notes}

• “No study is perfect: sample size, confounders, and chance all leave room for error.”
- some studies are just bad.
• “Influencers and headlines or TikTok, don't leave room for nuance and often do not report what the study actually says.”  
• “we’ll learn more about scientific studies and stats basics so you can spot the signal behind the clickbait.”  

:::

---

## Overview 


1. **The Research Question**  
2. **Study Population**  
3. **Study Design**  
4. **Paper Structure**  
5. **Reporting Metrics**
6. **Interpreting Results**  
7. **Controlling Confounders**  
8. **Misleading Studies & Headlines**

::: {.notes}
* **1 The Research Question** – What is the study trying to answer? 
* **2 Study Population** – Who or what was studied?  
* **3 Study Design** – How was the evidence gathered and analysed?  
* **4 Paper Structure** – Where to find key information.  
* **5 Reporting Metrics** – Common measures in studies.  
* **6 Interpreting Results** – P-values, confidence intervals, statistical significance.  
* **7 Controlling Confounders** – Adjustment, confounding and a little math.  
* **8 Misleading Studies & Headlines** – Typical pitfalls you’ll spot in the wild.
:::

---

## The Research Question

::: {.smaller2}

**Main goal of the study?**

- Drives study design, statistics, sample size  
- Clarify *primary* vs. *secondary* outcomes

<br>

::: {.fragment}

**Example (published RCT)**  
"We tested the hypothesis that long-term supplementation with omega-3 fatty acids would reduce cardiovascular events in this population."  
— [NEJM, 2012](https://www.nejm.org/doi/10.1056/NEJMoa1203859)

:::
:::

::: {.notes}
**What is the study trying to answer?**  
Every paper begins with a research question or hypothesis.  
That single goal shapes the design, determines which statistics are run, and dictates how many participants are needed.


**Primary vs. secondary outcomes**  
*Primary* = the outcome the trial is powered to detect.  
*Secondary* = extra measurements, often exploratory.  
Important: An exploratory study can be valuable, but don’t confuse it with a trial designed to prove causality.

:::

---

## The Research Question

::: {.smaller2}

### Questions to Ask

- What single question is the study trying to answer?  
- Is the **primary outcome** clearly stated—and the right one for that question?  
- Are **secondary outcomes** labelled as exploratory, or being oversold?  
- Does this question matter to **you / your patients**?

:::

---

## Who or What Is Being Studied?

**Population matters for relevance**  
_Not all studies are done on humans, and not all humans are like you._

::: {.fragment}

- **Cells (in vitro)**
- **Animals (in vivo)**
- **Humans (in vivo)**

:::

::: {.notes}
**Key idea**  

**The subjects of the study affect how applicable the results are.**  

The subjects tell you how far the findings can travel—cells → animals → humans is a “mechanistic → translational → applied” ladder.

**Cells (in vitro)**  
• Isolated cells in dishes or test tubes.  
• Great for uncovering biological mechanisms, drug targets, or toxicity pathways.  
• Cannot predict whole-body effects on their own.

**Animals (in vivo)**  
• Usually mice or rats; sometimes larger mammals.  
• Provide whole-organism context and dosing clues.  
• Differences in metabolism and lifespan still limit direct translation to people.

**Humans (in vivo)**  
• Ultimately most relevant, but generalisability varies by:  
  – Age, sex, health status  
  – Baseline diet, lifestyle, genetics  
• Always ask: “How close is this cohort to me or my patients?”

**Illustrative mismatch**  
A high-protein diet trial on *elderly Japanese women with osteoporosis undergoing survival training* probably says little about a 35-year-old office worker in Madrid.

**Take-home line**  
Before judging a result, check *who* or *what* was studied and whether that population resembles the one you care about.
:::

---

## Who or What Is Being Studied? 

### Questions to Ask

- **Who or what** was actually studied?  
- Why did the researchers pick that population or model?  
- How similar are these subjects to **you or the group you care about**?

---

## Study Design

::: {.smaller2}

What a study can claim depends on its design.

<br>

:::: {.columns}

::: {.column .fragment
      style="width:33%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#6bc7ff;">Observational</span>  
• Cross-sectional  
• Prospective cohort  
• Case-control
:::

::: {.column .fragment
      style="width:30%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#fdd663;">Interventional</span>  
• RCT   
• Pre–post (single-arm)



:::

::: {.column .fragment
      style="width:28%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             line-height:1.2;"}

### <span style="color:#a1e56e;">Evidence Synthesis</span>  
• Systematic review    
• Meta-analysis   



:::
::::

:::

::: {.notes}
**Key message**  
- *Observational* → explores associations.  
- *Interventional* → can test causality.  
- *Synthesis* → pools or summarises evidence for stronger estimates.

**Observational**  
- **Cross-sectional** – snapshot at one point in time.  
- **Prospective cohort** – follow exposure over time to see who develops the outcome.  
- **Case-control** – compare people with vs. without the outcome, look backward for exposures.  

→ Good for prevalence, trends, hypothesis generation; cannot by themselves prove cause.

**Interventional**  
- **Pre–post / single-arm** – measure before vs. after in the same group.  
- **Randomized Controlled Trial** – participants randomly assigned; balances confounders.  

→ Designed to answer causal questions and estimate treatment effects.

**Evidence Synthesis**  
- **Systematic review** – structured search + critical appraisal of all relevant studies.  
- **Meta-analysis** – statistical pooling step inside a systematic review (if data allow).  

→ Top of the evidence hierarchy when built on well-done RCTs; still limited by the quality and consistency of those underlying studies.
:::

---

## Other Study Designs (reference) {.smaller2}

<br>

:::: {.columns}
::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Observational**

• Case-control – with vs without outcome  
• Case report / series – detailed look at few patients  
• Ecological – group-level data only  
• Retrospective cohort – past records follow exposure → outcome  

<br>

**Mixed / Natural**

• Longitudinal – same subjects over time  
• Natural experiment – exposure assigned by external factors  
:::

::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Interventional**

• N-of-1 trial – single participant, alternating treatments  
• Cross-over trial – each subject receives all treatments  
• Before–after study – compare pre- vs post-intervention  

<br>

**Synthesis**

• Systematic review – structured summary, no pooling  
• Umbrella review – review of systematic reviews  
:::
::::

::: {.notes}
This is a quick reference list.  
I’ll mention that these designs exist but won’t unpack them here; listeners can pause the replay or check the show notes if they’d like more detail.
:::

---

## Study Design

#### Sample Size & Power

- **Power (1 − β)** = chance to find the effect you’re hunting  
  *Usual target: 80 %*

- Grows with **bigger N, larger effect, lower noise**

::: {.fragment}

- Calculated *before* the trial to size it properly

:::
::: {.fragment}

- Set from the **minimum clinically important difference (MCID)** (ideally)

:::
::: {.notes}

### 1  What power really is  
* β = Probability of not rejecting the null **if** the pre-specified effect size is true.  
* 80 % is convention (β = 0.20) because it balances feasibility and false-negative risk. 90 % is used in large cardiovascular mega-trials.

### 2  How it’s calculated
Depends on four inputs:  
1) expected effect size 2) outcome variability 3) α (usually 0.05) 4) **N**.  
➜ change any one and power shifts.

### 3  Pitfalls of being under- or over-powered  
* **Under-powered** → high type-II error; anything found that *is* significant often over-estimates the true effect (Ioannidis 2005).  
* **Over-powered** → detect 2 mm Hg or 0.15 kg and call it “significant.” Clinical relevance?

### 4  Which designs quote power?  
* **RCTs** — CONSORT requires a sample-size/power statement in Methods.  
* **Large cohort studies** often justify N for rare-event precision.  
* Cross-sectional & case-control papers rarely show a power calc; they discuss post-hoc detectable effect instead.
Cohorts usually enroll as many people as practical and then analyse whatever number of events occur; they report detectable effect sizes afterward rather than prospectively ‘powering’ the study like an RCT.


:::


## Study Design

#### Sample Size & Power

- Too small → wide CIs, missed effects  
- Too large → trivial but 'significant' findings

::: {.fragment}

- **Rule-of-thumb study sizes**   
  • < 50 per arm = pilot / feasibility  
  • 50 – 300 per arm = typical nutrition RCT  
  • > 300 per arm = large, clinically robust

:::
::: {.fragment}

- Power targets the **primary** outcome; secondaries often under-powered

:::
::: {.notes}
### 5 Primary vs secondary outcomes  
* Power calc is done **for the primary endpoint only**.  
* Secondary outcomes are usually under-powered; a significant result there is hypothesis-generating unless pre-specified with multiplicity correction. The secondary outcomes could be adequately powered
A null secondary is only convincing when its CI is tight enough to rule out any benefit we’d care about; otherwise it’s just under-powered—and we should label it inconclusive, not negative.

### 6 Case-study hook (twin vegan RCT)  
* Only 22 twin pairs → designed as a pilot.  
* Authors admit they lacked power for insulin resistance, yet headlines still hyped a –30 % insulin claim.

> “When you hear ‘the study was under-powered,’ translate that to ‘big uncertainty and effect sizes that might shrink or vanish at scale.’”

> RCTs should at least justify the number of subjects in the trial. 

**Rule-of-thumb footnote**  
✦ Chow & Liu, *Design & Analysis of Clinical Trials*, Ch 3 – rough ranges, always adjust for variance.

:::


---

## Study Design 

::: {.smaller2}

### Questions to Ask

- What type of study is this — observational, interventional, or a synthesis?

- Can this design support a **causal claim**, or only association?  
- Is the control / comparison group appropriate?  
- For meta-analyses, are the pooled studies similar enough?  
- Was the sample size justified, and did they report a power calculation for the primary endpoint?


:::

---


## Structure of a Scientific Paper 


1. **Abstract**  
2. **Introduction**  
3. **Methods**  
4. **Results**  
5. **Discussion**  
6. **Conclusion**  
7. **Supplementary**

::: {.notes}
**Abstract** – One-paragraph snapshot: question, design, key findings, and take-home.  
**Introduction** – Background, gap in knowledge, explicit research question, and why it matters.  
**Methods** – Who/what was studied, how data were gathered, study design, statistics used; lets you judge rigour and reproducibility.  
**Results** – Objective numbers: tables, figures, raw effect sizes. No interpretation here.  
**Discussion** – Authors’ interpretation, context with past work, strengths, limitations, and speculation.  
**Conclusion** – Concise answer to the research question; sometimes folded into Discussion.  
**Supplementary** – Extra data, detailed protocols, code, or extended figures that wouldn’t fit in the main text.

*Talking cue:* Emphasise that Methods and Results are where the “meat” lives; if those sections are thin, proceed with caution.
:::


---

## Structure of a Scientific Paper 

### Questions to Ask

- Does the **Abstract** match what’s really in the paper?  
- Are the **Methods** detailed enough to judge rigor?  
- Are **Results** shown with raw numbers, effect sizes and confidence intervals?
- Are any **Results** missing or highlighted more than reasonably?  
- Do the **Discussion/Conclusion** stay within the data’s limits?

---

## How Results Are Reported

#### Population Metrics

::: {.fragment}
- **Counts** – raw tally
:::
::: {.fragment}
- **Percent / Proportion** – share of the population
:::
::: {.fragment}
- **Rates** – cases per person-time
:::
::: {.fragment}
- **Standardised rates** – adjusted for age / other factors
:::
::: {.fragment}
- **Incidence vs prevalence** – new vs total cases
:::


::: {.notes}
**Counts** – e.g. 500 heart-disease cases.  
**Percent / Proportion** – share of the population, e.g. 5 % prevalence.  
**Rates** – events per person-time, e.g. 10 per 1 000 person-years.  
**Standardised rates** – adjusted for age or other demographics to compare populations fairly.  
**Incidence vs. prevalence** – new cases in a period vs. total existing cases.
:::

---

## How Results Are Reported 

**Group difference**

- Percent change  
- Mean difference  

::: {.fragment}

**Risk comparison**

- Absolute risk (AR)  
- Relative risk (RR)  
- Odds ratio (OR)  
- Number Needed to Treat (NNT)  

:::
::: {.notes}
*Percent change* – relative shift from baseline (10% drop).  
*Mean difference* – raw unit gap (−5 mm Hg).  

*Absolute risk* – direct percentage-point change (2% → 1%).  
*Relative risk* – ratio of probabilities (RR 1.5 = 50% higher).  
*Odds ratio* – ratio of odds; close to RR when outcome is rare.  
*NNT* – number of people who must receive the treatment for one to benefit (100 means large trials needed).
:::

---

## How Results Are Reported 

### Time-to-Event Metrics

- **Incidence rate**  
- **Hazard ratio (HR)**  
- **Survival curves** 

::: {.fragment}


### Metrics in Meta-analyses

- Forest plot
- Standardized Mean Difference


:::

::: {.notes}
*Incidence rate* – new cases per person-time (5 per 1 000 py).  
*Hazard ratio* – relative event rate over time (HR 0.75 = 25 % lower).  
*Survival curves* – Kaplan–Meier plot of event-free probability.  

*Meta-analysis* – combines studies; forest plot shows each effect and pooled “diamond.”  
*SMD* – standardised mean difference; useful when studies use different units.
:::

---

## How Results Are Reported 

::: {.smaller2}

### Questions to Ask

- Which metric is used—count, rate, risk ratio, mean difference, etc.?  
- Is that metric appropriate, or could it mislead (e.g., relative vs. absolute)?  
- Does it answer the **main research question**?  
- Could the presentation exaggerate or downplay the finding?

:::

---

## P-values & Confidence Intervals

::: {.smaller2}

<br>

**Is this effect real? How exact is it?**

<br>

| Concept                                  | What it tells us                  | Quick example                                   |
|------------------------------------------|-----------------------------------|-------------------------------------------------|
| **Statistical significance** *(p-value)* | Chance vs. real effect?           | Supplement ↓ BP **5 mm Hg**, *p = 0.03*          |
| **Precision** *(95 % CI)*                | How exact is the estimate?        | 5 mm Hg (**CI −8 to −2**)                        |
:::

::: {.notes}
### Why we need these measures  

Studies usually measure a sample, not the entire population.
Random variation in the sample can produce differences, even if there’s no real effect.
If we had data from the full population, we wouldn’t need p-values or confidence intervals.

*P-values* test whether those differences are likely due to chance; *confidence intervals* show how precisely we’ve pinned down the effect size.

### Key definitions
*P-value* – probability of obtaining data this extreme **assuming** the null-effect model is true.  
*95 % CI* – range that would capture the true effect in ~95 out of 100 identical studies.

> **ASA informal definition**  
> “A p-value is the probability, under a specified statistical model, that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” — American Statistical Association, 2016

**Notes to hit verbally:**  
* Threshold test: conventionally *p < 0.05* marks ‘statistically significant,’ but the cutoff is arbitrary.  
* p-value ≠ probability the effect is real.  
* Narrow CI ⇒ precise estimate; wide CI ⇒ more uncertainty.  
* A huge sample can make trivial effects “significant”; always couple p with effect size & CI.  
* Once p is well below the cutoff (e.g., 0.0003), extra decimal places add no meaning.
:::

---

## Interpreting the p-value 

- **Threshold test** → typically **p < 0.05**  
- Smaller p → stronger evidence **against** chance  

::: {.fragment}

- A *p*-value means little unless you also know the **effect size**

:::
::: {.fragment}

- **p ≠ probability the result is true**

:::
::: {.fragment}

> “5 mm Hg decrease, p = 0.03" → significant

:::

::: {.notes}
• Threshold is conventional, not magical; 0.01 or 0.10 have been used historically.  
• Highly significant (p < 0.001) values are often rounded because extra zeros don’t change interpretation.  
• Origin of 0.05 and common misuses go here.  
• Mention that p alone can’t judge importance—need effect size & CI.
:::

___

## Statistical vs. Practical Significance

::: {.smaller2}

<br>

| Finding (effect)             | p-value | Interpretation                                |
|------------------------------|:---------:|-----------------------------------------------|
| −0.15 kg (12 wk)             | 0.001  | **Statistically significant**; clinically trivial |
| −4.5 mm Hg systolic BP       | 0.09   | **Not statistically significant**; could matter if real |

<br>

:::

::: {.notes}
* A small, precise change can be meaningless in practice (−0.15 kg won’t alter anyone’s health plan).  
* A larger, borderline-non-significant effect may still matter—study could be under-powered, so look at CI width.  
* Always pair the p-value with effect size **and** CI before making clinical or lifestyle recommendations.  
* *“Would I change a guideline or my own behaviour over this number?”*
:::


---

## How to Interpret Results 

::: {.smaller2}

### Questions to Ask

- Is the result statistically significant—and by **how much**?  
- What does the **confidence interval** say about size and precision?  
- Is the effect big enough to matter in real life?  
- If it’s borderline, could chance, small N, selective data processing or multiple testing explain it?

:::

---

## "Controlling for"? 

**Goal:** isolate the effect of one variable while accounting for others.

::: {.fragment}

> Coffee & Heart Disease  
> Control for **smoking** so coffee isn’t blamed for smokers’ risk.

:::

::: {.fragment}

How?  

**Include the other factors in the statistical model.**

:::
::: {.notes}
“When you read ‘we controlled for education, BMI, and smoking,’ it means those factors **were included as variables in the model** so we can ask:  
*If two people were identical on those factors, does coffee still matter?*”
!RCTs already balance groups. You generally do not 'control for' things in RCTs. 
:::

---

## "Controlling for"?

::: {.smaller2}

#### Example Cohort Study

**Question:** Does coffee raise heart-disease risk?

We record coffee cups/day, smoking, age, and heart-disease outcome.

<br> 

::: {.fragment}

**Model with controls**  
`Heart disease = Coffee + Smoking + Age + error`  
→ Estimates the coffee effect *independent of* smoking & age
:::
::: {.fragment}

&nbsp;

**Model without smoking**  
`Heart disease = Coffee + Age + error`  
→ Smoking still influences both coffee & disease → **confounding**

:::
:::

::: {.notes}
Leaving a major confounder out biases the coffee coefficient toward the smoking effect.
:::

---

## "Controlling for"?

::: {.smaller2}

### Under the Hood 

<br>

$$
Y = \alpha \;+\; \beta_1 X_{coffee} \;+\; \beta_2 X_{smoke} \;+\; \beta_3 X_{age} \;+\; \varepsilon
$$

| Symbol            | Plain English                                           |
|-------------------|---------------------------------------------------------|
| $Y$               | Outcome (heart disease)                                 |
| $\alpha$          | Baseline when all X = 0                                 |
| $\beta_{1,2,3}$   | Effect of each variable **holding the others constant** |
| $\varepsilon$     | Random noise / unexplained variation                    |

::: {.notes}
“The Greek letters just stand in for ‘effect of each predictor.’”
:::

:::

---

## “Controlling for”?


* Works **only** for variables we measured and included.  
* Unmeasured confounding can still bias results.  
* **Randomized trials** help by balancing both known and unknown confounders.

<small>More math? See the linked [PSU resource](https://online.stat.psu.edu/stat462/node/131/)</small>

::: {.notes}
“Adjustment is powerful but never perfect. Always ask what might be missing.”
RCTs already balance groups. Don’t plug in variables that changed after randomization (like calories eaten); you’d be chopping away part of the diet’s real effect.
If you really want to know the effect minus weight-loss/calorie change, plan a special mediation analysis in advance; otherwise report the plain intention-to-treat result.

:::


---

## "Controlling for"?

::: {.smaller2}

### Questions to Ask

- Did the study adjust for the **key known confounders**?  
- What important factors might be missing or unmeasured?  
- Is the adjustment method explained and sensible?  


:::

::: {.notes}
JAMA study:
“Because it’s an RCT, the groups were comparable at baseline. The ~200-calorie deficit happened after allocation, so it’s part of how the vegan diet worked, not a bias that needs adjustment. If you tried to ‘control’ for those calories, you’d be stripping out some of the very effect you want to measure.”

:::

---

## Studies Can Mislead


- **Small samples inflate effects**  
- **Placebo isn’t inert**  
- **Complex Models & Subgroups Analyses**  
- **Funding or pet theories**
- **Multiple comparisons (p-hacking)**  
- **Self-reported or recall data**
- **Lack of blinding**
- **Surrogate ≠ clinical outcome** 

::: {.notes}
**Small samples inflate effects** – Big swings occur by chance; replication shrinks them.  
**Placebo isn’t inert** – “Control” oils, shakes, or pills often have biological activity.  
**Over-fitted/fragmented models** – Too many variables or subgroup dives can turn noise into a ‘finding.’  
**Funding / pet theories** – Sponsor or author bias can frame questions or spin conclusions; doesn’t invalidate, but requires scrutiny.
**Multiple comparisons / p-hacking** – Testing dozens of outcomes raises the odds of a “significant” result by luck; preregistration helps curb this.  
**Self-reported or recall data** – Diet, exercise, or symptoms often logged from memory; measurement error can blur real associations.
**Lack of blinding** – Open-label nutrition trials invite behaviour or reporting bias (“I’m on the ‘healthy diet,’ so I drop the afternoon soda”).  
**Surrogate ≠ clinical outcome** – LDL, insulin, DNA-methylation are useful markers, but they aren’t heart-attack counts or lifespan. A surrogate can move while the hard outcome stays flat (ACCELERATE trial, 2016).


:::

---

## Influencers Do Mislead 


- **One study ≠ truth**  
- **Click-bait abstracts**  
- **Publication bias**  
- **Petri-dish hype**  
- **Diet ≠ morality**  
- **Relative risk without absolute numbers**  
- **Cherry-picked subgroups** 
- **Association ≠ causation**


::: {.notes}
**One study ≠ truth** – Single papers are rarely definitive; replication matters.  
**Click-bait abstracts** – Summaries often overstate findings versus the body text.  
**Publication bias** – Null results stay in file drawers; positive ones get published. 
Perhaps you'd take fewer supplements if all the no-effect studies were published.
**Petri-dish hype** – Cell/animal results touted as human breakthroughs.  
**Diet ≠ morality** – Health claims sometimes smuggle in ideology or identity.  
**Relative risk without absolutes** – “50 % higher risk!” might be 2 cases → 3 cases per 10 000; influencers omit the baseline.
**Cherry-picked subgroups** – Headline highlights “80 % risk drop in women between 30 and 40” but ignores that 10 other subgroups showed nothing; classic media spin.
**Association ≠ causation** – Correlation alone can prompt fear or fads without causal proof. ie associations aren't the same level of evidence as causations. 
:::

---

## Core Questions to Ask 

::: {.smaller2}

<div style="font-size: 125%; line-height: 1.4;">

1. **Is the main question clear—and is it the one you care about?**  
2. **Study type:** can it speak to cause, or only association?  
3. **Who / what was studied—and how close is that to you?**  
4. **Metric chosen:** what does it reveal and what does it hide?  
5. **Result strength:** big, precise, and practically useful?  
6. **Red flags:** stats tricks, sensational claims, or conflicts?  

</div>

:::

::: {.notes}

1. Question clarity drives everything; if the paper drifts, stop.  
2. RCT or Mendelian randomization can test cause; cohort mainly spots links.  
3. Species, age, sex, health status—don’t apply mouse data to humans or elite-athlete data to office workers.  
4. Relative vs. absolute risks, mean vs. median—each tells a different story.  
5. Look at effect size *and* CI; a “significant” 0.15 kg loss isn’t life-changing.  
6. Tiny n, p-hacking, funding bias, or glitzy headlines demand extra caution.
[Hyped headlines](https://www.allinahealth.org/allina-news/2023/11/new-research-says-vegan-diets-are-good-for-the-heart)
:::

---

## Three Things to Remember {.smaller}

<div style="font-size: 127%; line-height: 1;">

1. **RCTs are the gold-standard for causality**  
   _Meta-analyses of well-run RCTs are even stronger._

<br>

2. **Cell & animal work is a first step, not a human verdict**
   _Petri-dish breakthroughs rarely translate 1:1 to people._
   
<br>

3. **Peek under the statistical hood**  
   _A result is only as strong as the methods and statistics behind it._

</div>

<div style="font-size:85%; color:#aaa; margin-top:0.8em;">
Good science takes longer than a 30-second TikTok.
</div>

::: {.notes}
Point 1 – “Gold-standard” avoids the absolute “only”; causal hints can come from natural experiments but need stronger assumptions.  
Point 2 – Petri-dish breakthroughs rarely translate 1:1 to people.  
Point 3 - headlines rarely mention the model, adjustments, or error checks, yet those determine whether a finding will hold up.


:::

# Follow-up on Vegan Twin Study

::: {.notes}
**Follow-up work the Stanford team did after the 8-week twin RCT**

• **Nutrient-cost analysis** – Using the same delivered-meal menus, they priced all ingredients at U.S. retail averages.  
  – Vegan meals cost ≈ $1.50 less per person per day (about 14 % cheaper) than the omnivore menus.  
  – Published as a short “Research Letter” in *JAMA Network Open* (Feb 2024).

• **Epigenetic-age sub-study** – They thawed the twins’ stored blood and ran three DNA-methylation clocks (Horvath, PhenoAge, GrimAge).  
  – Vegan arm showed a –1.9-year change in PhenoAge vs –0.3 years in omnivore (Δ ≈ –1.6 y, *p*≈0.04).  
  – Reported in *BMC Medicine* (2024); authors called it “exploratory.”

**How the media mashed it up**

• *New York Post* 29 Jul 2024 headline:  
  **“2 months of eating this diet knocks years off your biological age.”**  
  – Copy lifts the –1.9-year figure from the *BMC Medicine* paper.  
  – Mentions the identical-twin setup but ignores the small sample, short duration, and that the meals were chef-prepared.  
  – No reference to LDL-C, weight loss, or the true cost difference.

• [*New York Post* 22 Nov 2024 headline](https://nypost.com/2024/11/22/lifestyle/vegan-diet-can-make-you-wealthy-in-addition-to-healthy-study-finds/):  
  **“Vegan diet can make you wealthy, in addition to healthy, study finds.”**  
  – Pulls the grocery-cost analysis ($1.50/day cheaper) and frames it as a wealth booster, again skipping sample size and duration caveats.

Take-away you’ll voice:  
“Secondary analyses can be interesting, but headlines often elevate them above the primary outcomes—and leave out all the limitations.”
:::


