---
title: "Decoding the Hype"
subtitle: "Identifying real evidence in health & nutrition studies"
author: "John (The John & Calvin Podcast)"
format:
  revealjs:
    self-contained: true
    theme: [night]
    slide-number: true
    css: styles.css
---


## Health and nutrition research is messy.

::: {.fragment}

* No study is flawless. Every design has limits.

:::
::: {.fragment}

* Headlines & influencers turn nuance into **clickbait**.  

:::
::: {.fragment}

* **The Fix:**    
study design • basics stats • common pitfalls  


:::

::: {.notes}

• “No study is perfect: sample size, confounders, and chance all leave room for error.”
- some studies are just bad.
• “Influencers and headlines or TikTok, don't leave room for nuance and often do not report what the study actually says.”  
• “we’ll learn more about scientific studies and stats basics so you can spot the signal behind the clickbait.”  

:::

---

## Overview 


1. **The Research Question**  
2. **Study Population**  
3. **Study Design (Power)**  
4. **Study Structure**  
5. **How Results Are Reported**
6. **Interpreting Results**  
7. **~~Controlling for Other Factors~~**  
8. **Misleading Studies & Influencers**

::: {.notes}
* **1 The Research Question** – What is the study trying to answer? 
* **2 Study Population** – Who or what was studied?  
* **3 Study Design** – How was the evidence gathered and analysed?  
* **4 Study Structure** – Where to find key information.  
* **5 How Results Are Reported** – Common measures in studies.  
* **6 Interpreting Results** – P-values, confidence intervals, statistical significance.  
* **7 Controlling for Other Factors** – Adjustment, confounding and a little math.  
* **8 Misleading Studies & Influencers** – Typical pitfalls you’ll spot in the wild.
:::

---

## The Research Question

**Main goal of the study**

- Drives study design, statistics, sample size  
- Includes **measured metric** (e.g., BP change, risk)

::: {.fragment}
- States the **primary outcome** (vs. exploratory or secondary)

:::

::: {.fragment .smaller2}

Best practice (especially in RCTs): pre-register outcomes and analysis plan

:::

::: {.fragment .smaller2}
**Example (published RCT)**  
"We tested the hypothesis that long-term supplementation with omega-3 fatty acids would reduce cardiovascular events in this population."  
— [NEJM, 2012](https://www.nejm.org/doi/10.1056/NEJMoa1203859)
:::

::: {.notes}
**What is the study trying to answer?**  
Every paper begins with a research question or hypothesis.  
That single goal shapes the design, determines which statistics are run, and dictates how many participants are needed.


**Primary vs. secondary outcomes**  
*Primary* = the outcome the trial is powered to detect.  
*Secondary* = extra measurements, often exploratory.  
Important: An exploratory study can be valuable, but don’t confuse it with a trial designed to prove causality.

:::

---

## The Research Question

::: {.smaller2}

### Questions to Ask

- What single question is the study trying to answer?  
- Is the **primary outcome** clearly stated and the right one for that question?  
- Are **secondary outcomes** labelled as exploratory, or being oversold?  
- Does this question matter to you?

:::

---

## Study Population

**Population matters for relevance**  
_Not all studies are done on humans, and not all humans are like you._

::: {.fragment}

- **Cells (in vitro)**
- **Animals (in vivo)**
- **Humans (in vivo)**

:::

::: {.notes}
**Key idea**  

**The subjects of the study affect how applicable the results are.**  

The subjects tell you how far the findings can travel—cells → animals → humans is a “mechanistic → translational → applied” ladder.

**Cells (in vitro)**  
• Isolated cells in dishes or test tubes.  
• Great for uncovering biological mechanisms, drug targets, or toxicity pathways.  
• Cannot predict whole-body effects on their own.

**Animals (in vivo)**  
• Usually mice or rats; sometimes larger mammals.  
• Provide whole-organism context and dosing clues.  
• Differences in metabolism and lifespan still limit direct translation to people.

**Humans (in vivo)**  
• Ultimately most relevant, but generalisability varies by:  
  – Age, sex, health status  
  – Baseline diet, lifestyle, genetics  
• Always ask: “How close is this cohort to me or my patients?”

**Illustrative mismatch**  
A high-protein diet trial on *elderly Japanese women with osteoporosis undergoing survival training* probably says little about a 35-year-old office worker in Madrid.

**Take-home line**  
Before judging a result, check *who* or *what* was studied and whether that population resembles the one you care about.
:::

---

## Study Population

### Questions to Ask

- **Who or what** was actually studied?  
- Why did the researchers pick that population or model?  
- How similar are these subjects to **you or the group you care about**?

---

## Study Design

::: {.smaller2}

### What a study can claim depends on its design.

<br>

:::: {.columns}

::: {.column .fragment
      style="width:33%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#6bc7ff;">Observational</span>  
• Cross-sectional  
• Prospective cohort  
• Case-control
:::

::: {.column .fragment
      style="width:30%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             margin-right:0.5em; line-height:1.2;"}

### <span style="color:#fdd663;">Interventional</span>  
• RCT   
• Pre–post (single-arm)



:::

::: {.column .fragment
      style="width:28%; font-size:85%;
             background:#1e1e1e; border:1px solid #444;
             border-radius:12px; padding:0.4em;
             line-height:1.2;"}

### <span style="color:#a1e56e;">Evidence Synthesis</span>  
• Systematic review    
• Meta-analysis   



:::
::::

:::

::: {.notes}
**Key message**  
- *Observational* → explores associations.  
- *Interventional* → can test causality.  
- *Synthesis* → pools or summarises evidence for stronger estimates.

**Observational**  
- **Cross-sectional** – snapshot at one point in time.  
- **Prospective cohort** – follow exposure over time to see who develops the outcome.  
- **Case-control** – compare people with vs. without the outcome, look backward for exposures.  

→ Good for prevalence, trends, hypothesis generation; cannot by themselves prove cause.

**Interventional**  
- **Pre–post / single-arm** – measure before vs. after in the same group.  
- **Randomized Controlled Trial** – participants randomly assigned; balances confounders.  

→ Designed to answer causal questions and estimate treatment effects.

**Evidence Synthesis**  
- **Systematic review** – structured search + critical appraisal of all relevant studies.  
- **Meta-analysis** – statistical pooling step inside a systematic review (if data allow).  

→ Top of the evidence hierarchy when built on well-done RCTs; still limited by the quality and consistency of those underlying studies.
:::

---

## Other Study Designs (reference) {.smaller2}

<br>

:::: {.columns}
::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Observational**

• Case-control – with vs without outcome  
• Case report / series – detailed look at few patients  
• Ecological – group-level data only  
• Retrospective cohort – past records follow exposure → outcome  

<br>

**Mixed / Natural**

• Longitudinal – same subjects over time  
• Natural experiment – exposure assigned by external factors  
:::

::: {.column width="48%" style="font-size:80%; line-height:1.35;"}

**Interventional**

• N-of-1 trial – single participant, alternating treatments  
• Cross-over trial – each subject receives all treatments  
• Before–after study – compare pre- vs post-intervention  

<br>

**Synthesis**

• Systematic review – structured summary, no pooling  
• Umbrella review – review of systematic reviews  
:::
::::

::: {.notes}
This is a quick reference list.  
I’ll mention that these designs exist but won’t unpack them here; listeners can pause the replay or check the show notes if they’d like more detail.
:::

---

## Study Design

#### Sample Size & Power

- **Power (1 − β)**: the probability of finding an effect if it really exists     
  *Usual target: 80%*

- Grows with **bigger N, larger effect, lower noise**

::: {.fragment}

- Calculated *before* the trial to size it properly

:::
::: {.fragment}

- Set from the **minimum clinically important difference (MCID)** (ideally)

:::
::: {.notes}

### 1  What power really is  
* β = Probability of not rejecting the null **if** the alternative hypothesis true.  
* 80 % is convention (β = 0.20) because it balances feasibility and false-negative risk. 90 % is used in large cardiovascular mega-trials.
80%? If there really is an effect, there's an 80% chance your study will detect it (i.e. produce a statistically significant result)
β is the chance you miss the effect (a false negative).
So 1 − β is the chance you catch it (a true positive).

### 2  How it’s calculated
Depends on four inputs:  
1) expected effect size 2) outcome variability 3) α (usually 0.05) 4) **N**.  
➜ change any one and power shifts.

### 3  Pitfalls of being under- or over-powered  
* **Under-powered** → high type-II error (false negative); and anything found that *is* significant often over-estimates the true effect (Ioannidis 2005).  
* **Over-powered** → detect 2 mm Hg or 0.15 kg and call it “significant.” Clinical relevance?

### 4  Which designs quote power?  
* **RCTs** — CONSORT requires a sample-size/power statement in Methods.  
* **Large cohort studies** often justify N for rare-event precision.  
* Cross-sectional & case-control papers rarely show a power calc; they discuss post-hoc detectable effect instead.
Cohorts usually enroll as many people as practical and then analyse whatever number of events occur; they report detectable effect sizes afterward rather than prospectively ‘powering’ the study like an RCT.


:::


## Study Design

#### Sample Size & Power

- Too small → wide CIs, missed effects  
- Too large → trivial but 'significant' findings

::: {.fragment}

- **Rule-of-thumb study sizes**   
  • < 50 per arm = pilot / feasibility  
  • 50 – 300 per arm = typical nutrition RCT  
  • > 300 per arm = large, clinically robust

:::
::: {.fragment}

- Power targets the **primary** outcome; secondaries often under-powered

:::
::: {.notes}
### 5 Primary vs secondary outcomes  
* Power calc is done **for the primary endpoint only**.  
* Secondary outcomes are usually under-powered; a significant result there is hypothesis-generating unless pre-specified with multiplicity correction. The secondary outcomes could be adequately powered
A null secondary is only convincing when its CI is tight enough to rule out any benefit we’d care about; otherwise it’s just under-powered—and we should label it inconclusive, not negative.

### 6 Case-study hook (twin vegan RCT)  
* Only 22 twin pairs → designed as a pilot.  
* Authors admit they lacked power for insulin resistance, yet headlines still hyped a –30 % insulin claim.

> “When you hear ‘the study was under-powered,’ translate that to ‘big uncertainty and effect sizes that might shrink or vanish at scale.’”

> RCTs should at least justify the number of subjects in the trial. 

**Rule-of-thumb footnote**  
✦ Chow & Liu, *Design & Analysis of Clinical Trials*, Ch 3 – rough ranges, always adjust for variance.

:::


---

## Study Design 

::: {.smaller2}

### Questions to Ask

- What type of study is this: observational, interventional, or a synthesis?

- Can this design support a **causal claim**, or only association?  
- Is the control / comparison group appropriate?  
- For meta-analyses, are the pooled studies similar enough?  
- Was the sample size justified, and did they report a power calculation for the primary endpoint?


:::

---


## Study Structure


1. **Abstract**  
2. **Introduction**  
3. **Methods**  
4. **Results**  
5. **Discussion**  
6. **Conclusion**  
7. **Supplementary**

::: {.notes}
**Abstract** – One-paragraph snapshot: question, design, key findings, and take-home.  
**Introduction** – Background, gap in knowledge, explicit research question, and why it matters.  
**Methods** – Who/what was studied, how data were gathered, study design, statistics used; lets you judge rigour and reproducibility.  
**Results** – Objective numbers: tables, figures, raw effect sizes. No interpretation here.  
**Discussion** – Authors’ interpretation, context with past work, strengths, limitations, and speculation.  
**Conclusion** – Concise answer to the research question; sometimes folded into Discussion.  
**Supplementary** – Extra data, detailed protocols, code, or extended figures that wouldn’t fit in the main text.

*Talking cue:* Emphasize that Methods and Results are where the “meat” lives; if those sections are thin, proceed with caution.
:::


---

## Study Structure

### Questions to Ask

- Does the **Abstract** match what’s really in the paper?  
- Are the **Methods** detailed enough to judge rigor?  
- Are **Results** shown with raw numbers, effect sizes and confidence intervals?
- Are any **Results** missing or highlighted more than reasonably?  
- Do the **Discussion/Conclusion** stay within the data’s limits?

---

## How Results Are Reported

#### Describing the Population

::: {.smaller2}

::: {.fragment}
- **Counts** – raw number (e.g. 500 cases)
:::

::: {.fragment}
- **Percent / Proportion** – share of population (e.g. 5%)
:::

::: {.fragment}
- **Incidence** – new cases in a time period (e.g. 100 new this year)
:::

::: {.fragment}
- **Prevalence** – all existing cases at a point in time (e.g. 5% of adults)
:::

::: {.fragment}
- **Rates** – events per person-time (e.g. 10 per 1,000 person-years)
:::

::: {.fragment}
- **Standardized rates** – adjusted for age or other factors to compare groups
:::

:::

::: {.notes}
These metrics describe how common a condition or event is in the study group.

We're calling them “population” metrics here, but in most studies, they’re calculated from a **sample**

Counts – e.g. 500 heart-disease cases  
Percent / Proportion – e.g. 5% with high blood pressure  
Incidence – new cases over time, e.g. 100 new cases this year  
Prevalence – all existing cases, e.g. 5% of adults have condition  
Rates – person-time approach, e.g. 10 cases per 1,000 person-years  
Standardized rates – adjusted for fair comparisons (e.g. by age)
:::

---

## How Results Are Reported 

#### Measuring Differences in Outcomes

::: {.smaller2}

::: {.fragment}
- **Mean difference** – raw difference (e.g. −5 mm Hg in blood pressure)
:::

::: {.fragment}
- **Median difference** – comparison of middle values (less sensitive to outliers)
:::


::: {.fragment}
- **Percent change** – relative difference from baseline or control         
  (e.g. treatment group: 180, control group: 200 → 10% lower in treatment)

:::

:::

::: {.notes}
Mean difference – absolute gap in outcomes (e.g. −5 mm Hg)  
Percent change – relative change compared to baseline or control (e.g. 10% lower)
:::

---

## How Results Are Reported

### Understanding and Comparing Risk

::: {.smaller2}

#### Describing Risk in One Group

::: {.fragment}
- **Absolute risk** – chance of outcome in one group (e.g. 1%)
:::

::: {.fragment}
- **Odds** – ratio of events to non-events (e.g. 1:9 odds = 10% chance)
:::

#### Comparing Risk Between Groups

::: {.fragment}
- **Absolute Risk Reduction (ARR)** – difference in risk (e.g. 2% → 1% = 1% ARR)
:::

::: {.fragment}
- **Relative Risk (RR)** – ratio of risk (e.g. 1% ÷ 2% = RR 0.5 = 50% lower risk)
:::

::: {.fragment}
- **Odds Ratio (OR)** – ratio of odds (e.g. OR 2 = twice the odds)
:::

::: {.fragment}
- **Number Needed to Treat (NNT)** – how many need treatment for one to benefit (e.g. NNT = 100)
:::

:::

::: {.notes}
Absolute risk – how likely is the outcome in one group  
Odds – event / non-event (less intuitive than risk)  
ARR – direct subtraction of risks (2% → 1%)  
RR – proportionate risk (50% less likely)  
OR – compares odds; close to RR when events are rare  
NNT – helpfulness of treatment: lower NNT = more effective
:::


---

## How Results Are Reported

### Tracking Outcomes Over Time

::: {.smaller2}

::: {.fragment}
- **Incidence rate** – new cases per person-time (e.g. 5 per 1,000 person-years)
:::

::: {.fragment}
- **Hazard ratio (HR)** – event rate over time (e.g. HR 0.75 = 25% lower risk)
:::

::: {.columns}

::: {.column width="57%"}
::: {.fragment}
- **Survival curves**        
  probability of staying event-free over time  
  (e.g. Kaplan–Meier plot)
  
:::
::: {.fragment}

  Kaplan–Meier analysis on the association of obesity 
  and malnutrition with the risk of all-cause death in
  hypertension patients. 
:::
:::

::: {.column width="43%"}
::: {.fragment}
[![](assets/survival_curve.png){width=100%}](https://pubmed.ncbi.nlm.nih.gov/36937935/)
:::
:::

:::

:::

::: {.notes}
Incidence rate – accounts for different follow-up time (e.g. 5 per 1,000 py)  
Hazard ratio – how quickly events occur over time (HR 0.75 = slower rate)  
Survival curves – visualize probability of avoiding event (e.g. death, relapse)
plot from **Obesity, malnutrition, and the prevalence and outcome of hypertension**

:::

---

## How Results Are Reported

### Summarizing Multiple Studies

::: {.smaller2}

::: {.fragment}
- **Standardized Mean Difference (SMD)** – compares effects when scales differ
:::

::: {.fragment}
- **Forest plot** – visual summary of study results and overall effect
:::


::: {.fragment}


[![](assets/Forest_plot.png){width=65%}](https://www.jstage.jst.go.jp/article/jmaj/8/2/8_395/_article/-char/ja/)

Forest plots of saturated fatty acid reduction trials on myocardial infarction.

No statistically significant reduction was noted.

:::


:::

::: {.notes}
Forest plot – shows each study’s result and pooled estimate (the “diamond”)  
SMD – used when studies report the same outcome using different units (e.g. depression scales)
:::

---

## How Results Are Reported 

::: {.smaller2}

### Questions to Ask

- Which metric is used: count, rate, risk ratio, mean difference, etc.?  
- Is that metric appropriate, or could it mislead (e.g., relative vs. absolute)?  
- Does it answer the **main research question**?  
- Could the metric exaggerate or downplay the finding?

:::

---


## Interpreting Results

#### P-values & Confidence Intervals

::: {.smaller2}

<br>

**Is this effect real? How precise is it?**

::: {.fragment}

<br>

| Concept                                  | What it tells us                  | Quick example                                   |
|------------------------------------------|-----------------------------------|-------------------------------------------------|
| **Statistical significance** *(p-value)* | Chance vs. real effect?           | Supplement ↓ BP **5 mm Hg**, *p = 0.03*          |
| **Precision** *(95 % CI)*                | How exact is the estimate?        | 5 mm Hg (**CI −8 to −2**)                        |
:::
:::

::: {.notes}
### Why we need these measures  

Studies usually measure a sample, not the entire population.
Random variation in the sample can produce differences, even if there’s no real effect.
If we had data from the full population, we wouldn’t need p-values or confidence intervals.

*P-values* test whether those differences are likely due to chance; *confidence intervals* show how precisely we’ve pinned down the effect size.

### Key definitions
*P-value* – probability of obtaining data this extreme **assuming** the null-effect model is true.  
*95 % CI* – range that would capture the true effect in ~95 out of 100 identical studies.

> **ASA informal definition**  
> “A p-value is the probability, under a specified statistical model, that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value.” — American Statistical Association, 2016

**Notes to hit verbally:**  
* Threshold test: conventionally *p < 0.05* marks ‘statistically significant,’ but the cutoff is arbitrary.  
* p-value ≠ probability the effect is real.  
* Narrow CI ⇒ precise estimate; wide CI ⇒ more uncertainty.  
* A huge sample can make trivial effects “significant”; always couple p with effect size & CI.  
* Once p is well below the cutoff (e.g., 0.0003), extra decimal places add no meaning.
:::

---

## Interpreting Results

#### The p-value 

- **Threshold test**: typically **p < 0.05**  
- Smaller p means stronger evidence **against** chance  

::: {.fragment}

- A *p*-value means little unless you also know the effect size

:::
::: {.fragment}

- **p ≠ probability the result is true**

:::
::: {.fragment}

> “5 mm Hg decrease, p = 0.03" → significant

:::

::: {.notes}
• Threshold is conventional, not magical; 0.01 or 0.10 have been used historically.  
• Highly significant (p < 0.001) values are often rounded because extra zeros don’t change interpretation.  
• Origin of 0.05 and common misuses go here.  
• Mention that p alone can’t judge importance—need effect size & CI.
:::

___

## Interpreting Results

#### Statistical vs. Practical Significance

::: {.smaller2}

::: {.fragment}

<br>

| Finding (effect)             | p-value | Interpretation                                |
|------------------------------|:---------:|-----------------------------------------------|
| −0.15 kg (12 wk)             | 0.001  | **Statistically significant**; clinically trivial |
| −8.5 mm Hg systolic BP       | 0.09   | **Not statistically significant**; could matter if real |

<br>

:::
:::

::: {.notes}
* A small, precise change can be meaningless in practice (−0.15 kg won’t alter anyone’s health plan).  
* A larger, borderline-non-significant effect may still matter—study could be under-powered, so look at CI width.  
* Always pair the p-value with effect size **and** CI before making clinical or lifestyle recommendations.  
* *“Would I change a guideline or my own behaviour over this number?”*
:::


---

## Interpreting Results

::: {.smaller2}

### Questions to Ask

- Is the result statistically significant and by **how much**?  
- What does the **confidence interval** say about size and precision?  
- Is the effect big enough to matter in real life?  
- If it's borderline, could high variation, small N, selective data processing or multiple testing explain it?


::: {.notes}
P-value: but does "**how much**" really matter? Maybe, maybe not.

Look at Result and Table 2 in vegan study, where is p-value? 

Some journals actually moving away from just reporting p-values, and focus on CI, to show magnitude, and not just 'sig/non-sig'.


:::
:::


---

## Core Questions to Ask 

::: {.smaller2}

<div style="font-size: 115%; line-height: 1.4;">

::: {.fragment}

1. **What is the main question?** Is it the one you care about?  

:::
::: {.fragment}

2. **Who / what (and how many!) was studied?** How relevant is that to you?  

:::
::: {.fragment}

3. **What kind of study is it and how strong is the design?** Does it suggest an association, test causation, or synthesize  evidence?

:::
::: {.fragment}

4. **What metric was used?** What does it reveal and what might it miss?  

:::
::: {.fragment}

5. **How strong are the results?** Are they statistically significant, precise, and practically meaningful?  

:::
::: {.fragment}

6. **Are there any red flags?** Statistical 'tricks', sensational claims, or conflicts of interest? 

:::

</div>

:::

::: {.notes}

1. Question clarity drives everything; if the paper drifts, it's exploratory.  
2. RCT or Mendelian randomization can test cause; cohort mainly spots links.  
3. Species, age, sex, health status—don’t apply mouse data to humans or elite-athlete data to office workers. A study of 10 people is low evidence compared to hundreds of people. 
4. Relative vs. absolute risks, mean vs. median—each tells a different story.  
5. Look at effect size *and* CI; a “significant” 0.15 kg loss isn’t life-changing.  
6. Tiny n, p-hacking, funding bias, or glitzy headlines demand extra caution.
P-hacking: [A peculiar prevalence of p values just below .05](https://pubmed.ncbi.nlm.nih.gov/22853650/)
[Hyped headlines](https://www.allinahealth.org/allina-news/2023/11/new-research-says-vegan-diets-are-good-for-the-heart)
:::

---

## Studies Can Mislead

::: {.smaller}

::: {.fragment}

- **Small samples inflate effects**  

:::

::: {.fragment}
- **Placebo isn’t inert**  
:::

::: {.fragment}
- **Publication bias**  
:::

::: {.fragment}
- **Funding or pet theories**  
:::

::: {.fragment}
- **Multiple comparisons (p-hacking), cherry-picking subgroups**  
:::

::: {.fragment}
- **Self-reported or recall data**  
:::

::: {.fragment}
- **Lack of blinding**  
:::

::: {.fragment}
- **Surrogate ≠ clinical outcome**  
:::

:::

::: {.notes}
**Small samples inflate effects** – Big swings occur by chance; replication shrinks them. One or two unusual outcomes can shift the average dramatically. outlier??  
**Placebo isn’t inert** – “Control” oils, shakes, or pills often have biological activity.  
**Publication bias** – Null results stay in file drawers; positive ones get published.     
Perhaps you'd take fewer supplements if all the no-effect studies were published.  
**Funding / pet theories** – Sponsor or author bias can frame questions or spin conclusions; doesn’t invalidate, but requires scrutiny.     
**Multiple comparisons / p-hacking** – Testing dozens of outcomes raises the odds of a “significant” result by luck; preregistration helps curb this.  
**Cherry-picked subgroups** – Headline highlights “80 % risk drop in women between 30 and 40” but ignores that 10 other subgroups showed nothing; classic media spin.     
**Self-reported or recall data** – Diet, exercise, or symptoms often logged from memory; measurement error can blur real associations.       
**Lack of blinding** – Open-label nutrition trials invite behavior or reporting bias (“I’m on the ‘healthy diet,’ so I drop the afternoon soda”).  
**Surrogate ≠ clinical outcome** – LDL, insulin, DNA-methylation are useful markers, but they aren’t heart-attack counts or lifespan. A surrogate can move while the hard outcome stays flat (ACCELERATE trial, 2016).


:::

---

## Influencers Do Mislead 

::: {.smaller}

::: {.fragment}

- **One study ≠ truth**  

:::
::: {.fragment}
- **Petri-dish (mechanism) hype**  
:::

::: {.fragment}
- **Healthy Diet ≠ morality**  
:::

::: {.fragment}
- **Relative risk without absolute numbers**  
:::

::: {.fragment}
- **Association ≠ causation**  
:::

::: {.fragment}
- **Anecdotes ≠ strong evidence**  
:::

::: {.fragment}
- **Claim comes from headline, not published study**  
:::

:::

::: {.notes}
**One study ≠ truth** – Single papers are rarely definitive; replication matters.    
**Petri-dish hype** – Cell/animal results touted as human breakthroughs.      
**Diet ≠ morality** – Health claims sometimes smuggle in ideology or identity.  
**Relative risk without absolutes** – “50 % higher risk!” might be 2 cases → 3 cases per 10 000; influencers omit the baseline.           
            
**Association ≠ causation** – Correlation alone can prompt fear or fads without causal proof. ie associations aren't the same level of evidence as causations. 

**Anecdotes** - Why: Influencers often swap in anecdotes where data should go. It’s persuasive, but misleading.

**Claims from Headlines** - many influencers build entire posts around a press release, news write-up, or even just a tweet, not the actual science.
:::

---

## Three Things to Remember {.smaller}

<div style="font-size: 105%; line-height: 1;">


1. **Know the study’s main question**  
   _It’s the foundation for understanding everything that follows._
   
   _Don’t rely on the headline / influencer._

<br>

::: {.fragment}

2. **Understand the study design**   
   _It defines what the results can tell you and how they should be interpreted._ 
   
   _Take into account the full body of evidence._

:::   
<br>

::: {.fragment}

3. **Don't skip the statistics**  
   _Results are only as strong as the methods and statistical analysis behind them._
   
   _Understanding the basics goes a long way._

:::

</div>


::: {.notes}

**Point 1 – Start with the study's main question**  
If you don't understand what the study was actually trying to answer, you can't judge whether the results are meaningful.  
Influencers and headlines often jump to bold conclusions that don’t match the research question or primary outcome — especially when the study has multiple endpoints or exploratory results.  
This is why the *real* question — not just what was reported — matters most.

**Point 2 – Know what kind of study you're looking at**  
Different study types can answer different kinds of questions.  
An RCT can test cause-and-effect, while an observational study can only suggest associations — no matter how dramatic the result sounds.  
And even RCTs have limitations: they can be underpowered, biased, or irrelevant to your population.  
Understanding the design sets the boundaries for what you can believe.

**Point 3 – Don't skip the statistics**  
The numbers behind the result — sample size, effect size, p-values, confidence intervals — tell you whether a claim is solid or just noise.  
Headlines rarely mention the uncertainty, the adjustments, or whether the result was actually powered to detect a difference.  
If the stats are shaky, the finding probably is too — even if it sounds exciting.

:::
# Follow-up on Vegan Twin Study

::: {.notes}
**Follow-up work the Stanford team did after the 8-week twin RCT**

• **Nutrient-cost analysis** – Using the same delivered-meal menus, they priced all ingredients at U.S. retail averages.  
  – Vegan meals cost ≈ $1.50 less per person per day (about 14 % cheaper) than the omnivore menus.  
  – Published as a short “Research Letter” in *JAMA Network Open* (Feb 2024).

• **Epigenetic-age sub-study** – They thawed the twins’ stored blood and ran three DNA-methylation clocks (Horvath, PhenoAge, GrimAge).  
  – Vegan arm showed a –1.9-year change in PhenoAge vs –0.3 years in omnivore (Δ ≈ –1.6 y, *p*≈0.04).  
  – Reported in *BMC Medicine* (2024); authors called it “exploratory.”

**How the media reported it**

• *New York Post* 29 Jul 2024 headline:  
  **“2 months of eating this diet knocks years off your biological age.”**  
  – Copy lifts the –1.9-year figure from the *BMC Medicine* paper.  
  – Mentions the identical-twin setup but ignores the small sample, short duration, and that the meals were chef-prepared.  
  – No reference to LDL-C, weight loss, or the true cost difference.

• [*New York Post* 22 Nov 2024 headline](https://nypost.com/2024/11/22/lifestyle/vegan-diet-can-make-you-wealthy-in-addition-to-healthy-study-finds/):  
  **“Vegan diet can make you wealthy, in addition to healthy, study finds.”**  
  – Pulls the grocery-cost analysis ($1.50/day cheaper) and frames it as a wealth booster, again skipping sample size and duration caveats.

“Secondary analyses can be interesting, but headlines often elevate them above the primary outcomes—and leave out all the limitations.”
:::


