---
title: "How to Read Health & Nutrition Studies"
subtitle: |
  *A Practical Framework for Understanding Studies*

author: |
  **John Slough**  
  *for* **The John & Calvin Podcast**

output:
  xaringan::moon_reader:
    self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false


---


```{r notes, echo=FALSE, warning=FALSE}
# rmarkdown::render("reading_studies.Rmd")

# pagedown::chrome_print("reading_studies.html")

```

class: inverse, center, large

```{css, echo=FALSE}


.footnote {
  font-size: 0.5em;
  color: #666;
}

```



# Health and nutrition research is messy.

--

# Health and nutrition headlines and influencers make it worse.

<hr style="margin-top: 2em; margin-bottom: 2em;">

--
# Understand how studies are designed.

--

# Understand basic statistics and common metrics.


---

### Overview

1. **The Research Question**  
   *What is the study trying to answer?*

2. **Study Design**  
   *How was the evidence gathered?*

3. **Study Population**  
   *Who or what was studied?*

4. **Structure of a Scientific Paper**  
   *Where to find key information*

5. **How Results Are Reported**  
   *Common Metrics in Studies*

6. **How to Interpret Results**  
   *P-values, confidence intervals, and statistical significance*

7. **What It Means to "Control for" Something**  
   *Confounding and adjustment: some math*

8. **Studies Can Mislead and So Do the Headlines**  
   *Common issues in studies*

---

## 1. The Research Question

**What Is the Main Goal of the Study?**  
- Every paper starts with a research question or hypothesis.  
- This goal drives the design, statistics, and sample size.

**Example from a published RCT**  
> ["we tested the hypothesis that long-term supplementation with n–3 fatty acids would reduce the rate of cardiovascular events in this population of patients."](https://www.nejm.org/doi/10.1056/NEJMoa1203859)  

--

**Primary vs. secondary outcomes**  
- **Primary** = outcome the study is *powered* to detect.  
- **Secondary** = extra measurements, often exploratory.

The main purpose of the study can be exploratory. But don't confuse that with a study testing causality.

---

## 1. The Research Question

### Questions to Ask

- What specific question is the study trying to answer?

- Is the primary outcome clearly stated and appropriate?

- Are the secondary outcomes treated as exploratory, or being oversold?

- Does the goal match what you care about?

---

## 2. Study Design – How the Evidence Is Gathered

A study's design determines what it can conclude: **observational** studies explore associations, **interventional** studies test causality, and **syntheses** combine evidence across studies.

--


```{r hide_table_border, results='asis', echo=FALSE}
cat("
<style>
  /* Remove the outer border from both Observational and Interventional tables */
  #tbl_obs,
  #tbl_int,
  #tbl_synth {
    border: none !important;
    border-top: 1px solid #ddd !important;  /* put back the top line */
    border-bottom: 1px solid #ddd !important;  /* put back the top line */

  }
</style>
")
```

```{r show_top_table_border, results='asis', echo=FALSE}
cat("
<style>
  /* Remove the outer border from both Observational and Interventional tables */
  #tbl_obs {
    border-top: 1.5px solid #333 !important;  /* put back the top line */
  }
</style>
")
```

```{r show_last_table_border, results='asis', echo=FALSE}
cat("
<style>
  /* Remove the outer border from both Observational and Interventional tables */
  #tbl_synth {
    border-bottom: 1.5px solid #333 !important;  /* put back the bottom line */
  }
</style>
")
```

```{r show_header_border, results='asis', echo=FALSE}
cat("
<style>
  #tbl_obs thead tr {
    border-bottom: 1px solid #666 !important;
  }
</style>
")

```

```{r study_design_obs, echo=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)

# your data
obs <- data.frame(
  Design        = c("Cross-sectional", "Prospective cohort", "Quasi-experimental"),
  Description   = c("Snapshot of exposure & outcome",
                    "Follow exposure over time",
                    "Natural assignment, no randomization"),
  `What it Shows` = c("Prevalence, correlations",
                      "How exposures relate to future outcomes",
                      "Policy impacts / structural differences")
)
colnames(obs) <- c("Design","Description","What it Shows")

# build the table
kbl(
  obs,
  caption    = "<strong style='font-size:1.05em;'>Observational Studies</strong>",
  escape     = FALSE,
  table.attr = "id='tbl_obs'"
) %>%
  kable_styling(
    full_width        = TRUE,
    font_size         = 16,
    bootstrap_options = c("striped","hover","condensed")
  ) %>%
  row_spec(1:nrow(obs), background = "#f3f8fd",extra_css  = "padding-top:4px; padding-bottom:4px;"
  ) %>%
  column_spec(1, width = "30%", bold = TRUE) %>%
  column_spec(2, width = "40%") %>%
  column_spec(3, width = "30%")


```

```{r spacer1, results='asis', echo=FALSE}
cat("<div style='margin-top: 12px;'></div>")
```

```{r study_design_int, echo=FALSE, warning=FALSE}

int <- data.frame(
  Design = c("Single-arm (pre-post)", "Randomized Controlled Trial"),
  Description = c("Everyone gets treatment; pre vs post",
                  "Random assignment to groups"),
  `What it Shows` = c("Tests before/after change, may suggest causality, lacks control group", "Causality, treatment effect")
)

kbl(int, caption = "<strong style='font-size:1.05em;'>Interventional Studies</strong>", col.names = NULL, escape = FALSE,
      table.attr = "id='tbl_int'") %>%
  kable_styling(full_width = TRUE, font_size = 16, bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(1:nrow(int), background = "#f4fdf3") %>%
  column_spec(1, width = "30%", bold = TRUE) %>%
  column_spec(2, width = "40%") %>%
  column_spec(3, width = "30%") %>%
  row_spec(nrow(int), extra_css = "border-bottom: none;")
```

```{r spacer2, results='asis', echo=FALSE}
cat("<div style='margin-top: 12px;'></div>")
```

```{r study_design_synth, echo=FALSE, warning=FALSE}

synth <- data.frame(
  Design = "Meta-analysis",
  Description = "Combines data from multiple studies",
  `What it Shows` = "Combined estimate of effect (stronger evidence)"
)

kbl(synth, caption = "<strong style='font-size:1.05em;'>Evidence Synthesis</strong>", col.names = NULL, escape = FALSE,
      table.attr = "id='tbl_synth'") %>%
  kable_styling(full_width = TRUE, font_size = 16, bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(1:nrow(synth), background = "#f7f3fd") %>%
  column_spec(1, width = "30%", bold = TRUE) %>%
  column_spec(2, width = "40%") %>%
  column_spec(3, width = "30%")

```

---

## 2. Study Design – How the Evidence Is Gathered

Other designs you may see:

<small>
• *Case-control* – compares people with vs without outcome (handy for rare diseases)  

• *Systematic review* – a structured narrative summary without statistical pooling  

• *Retrospective cohort* – looks back at existing data to follow exposure → outcome  

• *N-of-1 trial* – a single patient receives alternating treatments to compare effects  

• *Ecological study* – analyzes group-level data, not individuals 

• *Longitudinal study* – follows the same subjects over time (can be observational or interventional)</small>

---

## 2. Study Design – How the Evidence Is Gathered

### Questions to Ask

- What type of study is this — observational, interventional, or a synthesis?

- Can this design prove causality, or only suggest associations?

- Is the comparison group (if any) appropriate and meaningful?

- For meta-analyses: Were the included studies similar enough to combine?

---

## 3. Who or What Is Being Studied?

**The subjects of the study affect how applicable the results are.**  
Not all studies are done on humans — and not all humans are like you.

--

**Cells (in vitro)**  
- Lab studies on isolated cells  
- Good for understanding biological mechanisms  
   
--


**Animals (in vivo)**  
- Whole-body systems (usually mice or rats)  
- Useful for early testing

--


**Humans (in vivo)**  
- Most relevant, but consider:  
 - Age, sex, health status  
 - Baseline diet and habits

--
- the study population may be very different from you
- e.g.  A study tested a high-protein diet on elderly Japanese women with Osteoporosis undergoing survival training.

---

## 3. Who or What Is Being Studied?

### Questions to Ask

- What type of subjects were studied?  

- Why were they chosen?

- How relevant are they to you?

---


## 4. Structure of a Scientific Paper

1. **Abstract** – Summary of Paper

2. **Introduction** – Background info, Research Question, Why the study matters.  

3. **Methods** – How the study was set up, what data were collected, and how they were analyzed.

4. **Results** – Tables/figures with raw findings.  

5. **Discussion** – Authors' interpretation of results and why they are important.  

6. **Conclusion** 

7. **Supplementary** – Extra data, protocols, code.

---

## 4. Structure of a Scientific Paper

### Questions to Ask

- Does the Abstract summarize what is actually in the study?

- Does the Methods section clearly explain how the study was done?

- Is the data and statistics in the Results presented clearly?

- Does the Discussion align with the actual findings?

---


## 5. How Results Are Reported

### Common Effect Metrics

**Comparing Groups**  
- **Mean difference** – e.g., −5 mm Hg BP
- **Percent Change** – relative change from baseline (e.g., 10% decrease)

--

**Comparing risk**  
- **Absolute risk (difference) (AR)** – direct % point change (e.g., 2% → 1%)  
- **Relative risk (RR)** – proportional change in risk (e.g., RR = 1.5 = 50% higher risk)  
- **Number Needed to Treat (NNT)** – how many need the treatment for one person to benefit (e.g., NNT = 100)

---

## 5. How Results Are Reported

### Common Effect Metrics

**Time-to-Event Metrics**  
- **Incidence rate** – number of new cases per person-time (e.g., 5 per 1,000 person-years)  
- **Hazard ratio (HR)** – compares the rate at which events happen between groups over time (e.g., HR = 0.75 = 25% lower event rate)  
- **Odds ratio (OR)** – compares the odds of an event between groups (e.g., OR = 2 = twice the odds)  
- **Survival analysis (e.g., Kaplan–Meier curves)** – shows the probability of surviving (or avoiding an event) over time across groups

--

**Metrics in Meta-analyses**  
- **Forest plots** – show individual study effects and the overall pooled result.
  - pooled result is shown as a diamond: center marks the average effect, and its width shows the confidence interval

---

## 5. How Results Are Reported

### Questions to Ask

- What type of metric is reported (mean difference, risk, rate, etc.)?

- Does this metric directly answer the study’s main question?

- Could the way the result is presented exaggerate or downplay the real impact?

- What does the chosen metric reveal, and what might it obscure?


---

## 6. How to Interpret Results

Reported metrics are usually accompanied by p-values and confidence intervals to assess reliability.

Interpreting results depends on two concepts:

--

- **Statistical significance** - Is the result real, or just random chance?
   - assessed using a *p-value*

- **Precision** - How wide or narrow is the range of values where the true effect likely falls.
   - shown with a *confidence interval (CI)*

--

*Why do we need these statistical measures?*  

> Studies usually measure a **sample**, not the entire population.  
> Random variation in the sample can create differences, even if there’s no real effect.  

> If we had data from the full population, we wouldn’t need p-values or confidence intervals.


---

## 6. How to Interpret Results

### Statistical Significance and P-values

- **Statistically significant**: result is *unlikely to be due to random chance*.
--

- **P-value**: puts a number on that *unlikelihood* [1].
--

- **Ranges from 0 to 1**, the smaller the p-value, the stronger the indication that the difference is real (not random)


--

- **p = 0.05** is the standard threshold, so **p < 0.05** is statistically significant
--

- A p-value is paired with an effect estimate (e.g. mean difference)
--

- A p-value is **not** the probability that the result is real.
  - p = 0.02 does not mean a 98% chance the effect is true.

--

> The supplement reduced blood pressure by 5.0 mm Hg, with a p-value of 0.03.

> 0.03 is less than 0.05 so the result is "statistically significant"

<span class="footnote">1. We say "unlikely" because frequentist hypothesis testing starts by assuming **no real effect**. A p-value asks: *"If nothing is happening, how often would data this extreme arise by chance?"* It quantifies that unlikelihood under a no-effect assumption. It does **not** tell us the probability the effect is true (that would require a Bayesian† approach).<br>†Bayesian methods... not going there now.</span>

---

## 6. How to Interpret Results

### Statistical Significance and P-values


#### **Definition**  
> 
> "**Informally**, a p-value is the probability, under a specified statistical model, that a statistical summary of the data (e.g., the sample mean difference between two compared groups) would be equal to or more extreme than its observed value."
> 
> — [American Statistical Association](https://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108)

--

Even more informally:

> Assuming no real difference between groups (the model), how likely is it to see a result at least as extreme as the one we got?

---

## 6. How to Interpret Results

### Statistical Significance and P-values


- **Statistically significant** is a **yes/no** label we apply after comparing the p&#8209;value to a **threshold**.

- **p < 0.05** is a common convention, but there is no mathematical or magical reason for it.

--

- [On the Origins of the .05 Level of
Statistical Significance](https://www2.psych.ubc.ca/~schaller/528Readings/CowlesDavis1982.pdf)

- [Misuse of p-values](https://en.wikipedia.org/wiki/Misuse_of_p-values)

--

Note:
> You’ll often see p < 0.01 or p < 0.001 instead of exact values.

> Once a result is highly statistically significant, the exact number (e.g. p = 0.0002134) doesn’t change the interpretation.

> Researchers just round it for simplicity.

---

## 6. How to Interpret Results

### Confidence Intervals

- **Confidence Interval (CI)**: the range where the *true effect* likely falls.


- A **95% CI** means: the statistical method used would capture the true effect in about 95 out of 100 similar studies.


- A **narrow CI** = more precise estimate  


- A **wide CI** = more uncertainty / variability 


- If the CI crosses the "no-effect" line (0 or 1), the result is *not* statistically significant.  

--

> Example: −5.0 mm Hg (95% CI −8.2 to −1.8) -> likely a real but variable effect  
> Example: −2.0 mm Hg (95% CI −4.5 to 0.5) -> likely no real effect


<small>Not all CIs are for statistical testing, some just show precision. 
When comparing groups using a 95% CI, crossing 0 (or 1 for ratios) means *p* > 0.05, not statistically significant.</small>

---

## 6. How to Interpret Results

### Statistical vs. Practical Significance

- Statistical significance does **not** imply practical significance

- *Example:* "12 weeks on a supplement led to a 0.2 kg weight loss (p = 0.001)" 

- This result is strongly statistically significant, but does 0.2 kg matter?


---

## 6. How to Interpret Results

### Questions to Ask

- Is the result statistically significant, and how close is the p-value to the 0.05 threshold?

- What does the confidence interval tell me about the size and precision of the effect?

- If the result is statistically significant, is the result practically important?

- If the result is barely significant, could it be due to chance, small sample size, selective reporting, or fishing through lots of outcomes?

---

## 7. What Does It Mean to "Control for" Something?

We often hear a study "controls for" something, e.g. education or smoking.

--

To **control for** (or **adjust for**) something means trying to isolate the effect of one variable, while accounting for other influences.

> Example: Example: A study on coffee and heart disease "controls for smoking," because smoking is associated with both higher coffee consumption and higher heart disease risk, making it hard to know if coffee itself is the real cause.

--

How do they do this?

- Statistically, this is done using mathematical **models that include the confounding variables**.

- The model estimates the effect of each variables **while holding the others constant**.  


---


## 7. What Does It Mean to "Control for" Something?

Research Question: **Does coffee increase heart disease?**

We already know smoking and age increases heart disease.

--

In our **prospective cohort study**, we collect data for coffee intake, smoking, age, and heart disease.

--

Model:
> `Heart Disease = Coffee + Smoking + Age + noise`  


We want to know: Does coffee increase heart disease, **without the influence of smoking and age**?



Including smoking and age in the model allows us to answer this question.


---

## 7. What Does It Mean to "Control for" Something?


**If we don't control for smoking**:


Model:
> `Heart Disease = Coffee + Age + noise`  


Smoking influences both coffee intake *and* heart disease.


Leaving it out means the model might wrongly blame **coffee** for effects actually due to **smoking**.

--

This is called **confounding**: a third variable confuses the apparent relationship between two variables.

---

## 7. What Does It Mean to "Control for" Something?

## **MATH**

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \varepsilon
$$

- $Y$ = the outcome (e.g., heart disease)  
- $X_1$ = coffee intake  
- $X_2$ = smoking  
- $X_3$ = age  
- Each $\beta$ estimates the effect of that variable, **controlling for** the others  
- $\varepsilon$ = leftover unexplained variation (the “noise”)

--

<small>
Controlling is a way to **statistically adjust for confounders**.  
It only works for variables that are **measured and included**. Unmeasured confounding can still bias results.  

>**Randomized trials** help by balancing both known and unknown confounders across groups.
</small>

---

## 7. What Does It Mean to "Control for" Something?

### Questions to Ask

- Did the study adjust for major known confounders (like smoking, age, BMI)?

- Are there important factors they might have missed or not measured?

- Is the adjustment described clearly, or does it seem vague or incomplete?

- Was the study randomized? (If yes, that helps handle unknown confounders too.)

---

## 8. Studies Can Mislead and So Do the Influencers

Even well-done studies, and the headlines about them, can be misleading.
Here are common issues to watch for when evaluating claims.


**Small Sample Size Exaggeration**
- Small studies often report bigger effects purely by random chance. Replication often shrinks or remove these effects.

**Placebo Problems**  
- Nutrition "controls" (like oils, shakes) often aren’t truly neutral.

**Overcomplicated Data Anlaysis and Models**  
- Convoluted stats with multiple sub-groups can mask weak findings.

**Pet Theories & Funding Conflicts**  
- Authors or funders may subtly tilt conclusions to support their interests. It doesn't invalidate a study, but it's worth noting.


---

## 8. Studies Can Mislead and So Do the Influencers

**One Study Isn’t the Whole Story**  
- One study is just a start. It should be replicated.

**Clickbait Abstracts**  
- Even scientific studies fall prey to the clickbait sickness. Look at the Methods and Results. Sometimes they're quite different.

**Publication Bias**  
- Studies with negative results often never get published. If they were, perhaps you'd take fewer supplements.

**Exaggerated Claims from Cell-based Studies**
- Good for the grift, bad for integrity. 

**Diet as Religion / Morality**
- This should be separate from its health impact.

---

class: inverse, larger



## <div style="text-align: center; font-size: 1.6em;">Core Questions to Ask</div>


- Is the study’s main question clear, and does it match what you care about?
<br>

- What type of study is it, and can it test cause and effect?
<br>

- Who or what was studied, and are the results relevant to you?
<br>

- What does the chosen metric reveal, and what might it obscure?
<br>

- How strong, precise, and practically meaningful are the results?
<br>

- Are there signs of statistical shenanigans?

---

class: inverse



# <div style="text-align: center; font-size: 1.2em;">If You Only Remember 3 Things</div>

<br>

### **1**. Only RCTs (and meta-analyses of RCTs) can establish causality.

<br>

### **2**. Cell and animal studies often do not translate to humans.

<br>

### **3**. Understanding basic statistics and reported metrics is necessary.


